{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Disaster News SVM classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMiwLzH3szjSOaDXmlgVr02",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albinjohn366/Disaster_Message_Classification/blob/Disaster_News_Classification_Vectorization/Disaster_News_SVM_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifVyGuE5XbTk"
      },
      "source": [
        "# Disaster Related News Classification\r\n",
        "Using Machine Learning tool to categorize Twitter news into disaster related and not disaster related. Using labelled dataset to train the model. Using Support Vector Machine for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-KmmsdwXVLj"
      },
      "source": [
        "import nltk\r\n",
        "import pandas as pd\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn import svm, naive_bayes\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "stopwords = stopwords.words('english')\r\n",
        "data = pd.read_csv('/disaster_response_messages_training.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQowb5rWdkUH"
      },
      "source": [
        "# Checking for inappropriate values\r\n",
        "index = data[data['related'].isin([1, 0]) == False].index\r\n",
        "data.drop(index=index, inplace=True)"
      ],
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz-poXn8Gnu5"
      },
      "source": [
        "# Function to find the type of word\r\n",
        "lemattizer_dict = {'V': wordnet.wordnet.VERB,\r\n",
        "                   'J': wordnet.wordnet.ADJ,\r\n",
        "                   'R': wordnet.wordnet.ADV}\r\n",
        "def find_type(string):\r\n",
        "  return lemattizer_dict.get(string, wordnet.wordnet.NOUN)\r\n"
      ],
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8TGV65bJfi3"
      },
      "source": [
        "# Function to lemitize\r\n",
        "def lemattize(message):\r\n",
        "  lemattizer = nltk.WordNetLemmatizer()\r\n",
        "  return [lemmatizer.lemmatize(token, find_type(typ[0])) for (token, typ) in nltk.pos_tag(nltk.word_tokenize(message)) \r\n",
        "    if (token.isalpha() and token not in stopwords)]"
      ],
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6mXFsY09kGB"
      },
      "source": [
        "# Function to convert strings to useful format\r\n",
        "words = []\r\n",
        "def convert_string(messages):\r\n",
        "  temp = []\r\n",
        "  for message in messages:\r\n",
        "    result = lemattize(message)\r\n",
        "    words.extend(result)\r\n",
        "    temp.append(str(result))\r\n",
        "  return temp"
      ],
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2yrLUj4YVz0"
      },
      "source": [
        "# Creating train and test data\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(data['message'], data['related'], test_size=0.3)\r\n",
        "x_train = convert_string(x_train)\r\n",
        "x_test = convert_string(x_test)"
      ],
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arl3jx64Zejp"
      },
      "source": [
        "# Using term frquency inverse document (TFIDF)\r\n",
        "vectorizer = TfidfVectorizer()\r\n",
        "vectorizer.fit(words)\r\n",
        "x_train_vc = vectorizer.transform(x_train)\r\n",
        "x_test_vc = vectorizer.transform(x_test)"
      ],
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Quyr4T23FY5j",
        "outputId": "05795732-6708-4b79-c389-942e831987fe"
      },
      "source": [
        "# Printing the vectorozed data\r\n",
        "print(x_train[0])\r\n",
        "print(x_train_vc[0])"
      ],
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Based', 'on', 'the', 'current', 'predication', 'local', 'should', 'forbid', 'boat', 'to', 'sail', 'out', 'and', 'should', 'assist', 'boat', 'owner', 'on', 'find', 'safe', 'anchorage']\n",
            "  (0, 21476)\t0.09576920996174329\n",
            "  (0, 21201)\t0.08268331897972223\n",
            "  (0, 19319)\t0.35574019762712417\n",
            "  (0, 18420)\t0.25409920261744434\n",
            "  (0, 18394)\t0.19061792273210237\n",
            "  (0, 16544)\t0.28347557283537433\n",
            "  (0, 15377)\t0.24735098595798832\n",
            "  (0, 15260)\t0.1612415525141724\n",
            "  (0, 15019)\t0.2529563067824314\n",
            "  (0, 12335)\t0.16971801979200532\n",
            "  (0, 7826)\t0.28347557283537433\n",
            "  (0, 7571)\t0.15329951477392242\n",
            "  (0, 4905)\t0.2018887435777399\n",
            "  (0, 2395)\t0.4007334303820894\n",
            "  (0, 1840)\t0.2516033127416185\n",
            "  (0, 1340)\t0.19584902894210332\n",
            "  (0, 834)\t0.09477993091118236\n",
            "  (0, 832)\t0.28347557283537433\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRPYnUYJRzJV"
      },
      "source": [
        "# Creating the classifier\r\n",
        "clf = svm.SVC(kernel='linear', gamma='auto', C=1).fit(x_train_vc, y_train)"
      ],
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r82goXKWSQti",
        "outputId": "500c5978-c261-4b94-9328-1aa7a54d3908"
      },
      "source": [
        "# Predicting for the test elements and printing the accuracy\r\n",
        "predictions = clf.predict(x_test_vc)\r\n",
        "print(accuracy_score(predictions, y_test))"
      ],
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8245530012771393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5Ry26cmUY1S"
      },
      "source": [
        "### Result from SVM model\r\n",
        "It is observed that linear kernal has the best results. The accuracy was 10% more for linear kernal compared to poly and rbf.\r\n",
        "\r\n",
        "*Kernel*\r\n",
        "\r\n",
        "kernel parameters selects the type of hyperplane used to separate the data. Using ‘linear’ will use a linear hyperplane (a line in the case of 2D data). ‘rbf’ and ‘poly’ uses a non linear hyper-plane\r\n",
        "\r\n",
        "*gamma*\r\n",
        "\r\n",
        "gamma is a parameter for non linear hyperplanes. The higher the gamma value it tries to exactly fit the training data set\r\n",
        "\r\n",
        "*C*\r\n",
        "\r\n",
        "C is the penalty parameter of the error term. It controls the trade off between smooth decision boundary and classifying the training points correctly.Increasing C values may lead to overfitting the training data. In this case, Increasing C resulted in less accuracy due to overfitting. Reduced C also caused less accuracy due to increased error. C = 1 is the apt one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqRsISzpeXq8",
        "outputId": "732367ee-c903-4b47-bb9c-6ad7fcf5a8da"
      },
      "source": [
        "# Trying the same with naive Bayes Model\r\n",
        "naive = naive_bayes.MultinomialNB()\r\n",
        "naive.fit(x_train_vc, y_train)\r\n",
        "predictions_for_naive = naive.predict(x_test_vc)\r\n",
        "print(accuracy_score(predictions_for_naive, y_test))"
      ],
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7755427841634738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "457qY-BtgGdn"
      },
      "source": [
        "### Result from Naive Bayes Model\r\n",
        "Naive bayes model took less time for fitting in the data. But SVM produced more accuracy compared to this model."
      ]
    }
  ]
}